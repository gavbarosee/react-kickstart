name: QA Automation

permissions:
  contents: write
  pull-requests: write
  issues: write

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  quick-qa:
    name: Quick QA Tests
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      - name: Run flag validation tests
        run: npm run test:flags

      - name: Run feature validation tests
        run: npm run test:features

      - name: Run quick QA tests
        run: |
          cd qa-automation
          npm install
          npm run test:quick

      - name: Upload quick test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: quick-test-results
          path: |
            qa-automation/test-report-*.json
            qa-automation/reports/
            reports/flag-validation-report-*.json
          retention-days: 3
          if-no-files-found: warn

  critical-qa:
    name: Critical Configuration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop')
    needs: quick-qa

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      - name: Generate test matrix
        run: |
          cd qa-automation
          npm install
          node test-matrix-generator.js

      - name: Run critical configuration tests

        run: |
          cd qa-automation
          npm install
          node test-runner.js critical 20

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: critical-test-results
          path: |
            qa-automation/test-report-*.json
            qa-automation/reports/
            reports/flag-validation-report-*.json
          retention-days: 7
          if-no-files-found: warn

      - name: Comment test results on PR
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // Find the latest test report
            const qaDir = path.join(process.cwd(), 'qa-automation');
            const reportsDir = path.join(qaDir, 'reports');

            let reportFiles = [];

            // Check both locations for test reports
            if (fs.existsSync(qaDir)) {
              const files = fs.readdirSync(qaDir);
              reportFiles = files.filter(f => f.startsWith('test-report-') && f.endsWith('.json'));
            }

            if (reportFiles.length === 0 && fs.existsSync(reportsDir)) {
              const files = fs.readdirSync(reportsDir);
              reportFiles = files.filter(f => f.startsWith('test-report-') && f.endsWith('.json'))
                .map(f => path.join('reports', f));
            }

            if (reportFiles.length === 0) {
              console.log('No test report found in qa-automation/ or qa-automation/reports/');
              return;
            }

            const latestReport = reportFiles.sort().pop();
            const reportPath = path.join(qaDir, latestReport);
            const report = JSON.parse(fs.readFileSync(reportPath, 'utf8'));

            const comment = `## ðŸ§ª QA Test Results

            **Critical Configuration Tests Summary:**
            - ðŸ“Š Total Tests: ${report.summary.total}
            - âœ… Successful: ${report.summary.successful}
            - âŒ Failed: ${report.summary.failed}
            - ðŸ“ˆ Success Rate: ${report.summary.successRate}
            - â±ï¸ Duration: ${report.summary.totalDuration}

            ${report.summary.failed > 0 ? `
            **Failed Tests:**
            ${report.failedTests.slice(0, 5).map(test => `- \`${test.testName}\`: ${test.error}`).join('\n')}
            ${report.failedTests.length > 5 ? `\n... and ${report.failedTests.length - 5} more` : ''}
            ` : 'ðŸŽ‰ All tests passed!'}

            <details>
            <summary>View detailed results</summary>

            \`\`\`json
            ${JSON.stringify(report.summary, null, 2)}
            \`\`\`
            </details>`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  comprehensive-qa:
    name: Comprehensive QA Tests
    runs-on: ubuntu-latest
    timeout-minutes: 120
    if: github.event_name == 'push' && github.ref == 'refs/heads/main' && contains(github.event.head_commit.message, '[comprehensive-qa]')
    needs: critical-qa

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      - name: Generate test matrix
        run: |
          cd qa-automation
          npm install
          node test-matrix-generator.js

      - name: Run comprehensive tests

        run: |
          cd qa-automation
          npm install
          node test-runner.js critical
          node test-runner.js standard 25
          node test-runner.js edge 10

      - name: Upload comprehensive test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-test-results
          path: |
            qa-automation/test-report-*.json
            qa-automation/reports/
          retention-days: 14
          if-no-files-found: warn

      - name: Create release comment
        if: success()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // Find all test reports from this run
            const qaDir = path.join(process.cwd(), 'qa-automation');
            const reportsDir = path.join(qaDir, 'reports');

            let reportFiles = [];

            // Check both locations for test reports
            if (fs.existsSync(qaDir)) {
              const files = fs.readdirSync(qaDir);
              reportFiles = files.filter(f => f.startsWith('test-report-') && f.endsWith('.json'));
            }

            if (fs.existsSync(reportsDir)) {
              const files = fs.readdirSync(reportsDir);
              const reportsSubdir = files.filter(f => f.startsWith('test-report-') && f.endsWith('.json'))
                .map(f => path.join('reports', f));
              reportFiles = [...reportFiles, ...reportsSubdir];
            }

            if (reportFiles.length === 0) {
              console.log('No test reports found in qa-automation/ or qa-automation/reports/');
              return;
            }

            // Get the latest reports (should be from this run)
            const recentReports = reportFiles.sort().slice(-3);
            let totalTests = 0;
            let totalSuccessful = 0;
            let totalFailed = 0;

            recentReports.forEach(file => {
              try {
                const report = JSON.parse(fs.readFileSync(path.join(qaDir, file), 'utf8'));
                if (report.summary) {
                  totalTests += report.summary.total || 0;
                  totalSuccessful += report.summary.successful || 0;
                  totalFailed += report.summary.failed || 0;
                }
              } catch (error) {
                console.log(`Error reading report file ${file}:`, error.message);
              }
            });

            const successRate = Math.round((totalSuccessful / totalTests) * 100);

            const comment = `## ðŸš€ Comprehensive QA Results

            **Full Test Suite Completed Successfully!**
            - ðŸ“Š Total Configurations Tested: ${totalTests}
            - âœ… Successful: ${totalSuccessful}
            - âŒ Failed: ${totalFailed}
            - ðŸ“ˆ Overall Success Rate: ${successRate}%

            ${successRate >= 95 ? 'ðŸŽ‰ **Ready for release!**' : 'âš ï¸ **Review failures before release**'}

            All critical, standard, and edge case configurations have been validated.
            The React Kickstart CLI is working correctly across all supported permutations.`;

            // Create an issue for tracking
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `QA Report: Comprehensive Testing - ${successRate}% Success Rate`,
              body: comment,
              labels: ['qa', 'testing', 'release-validation']
            });

  performance-benchmark:
    name: Performance Benchmark
    runs-on: ubuntu-latest
    timeout-minutes: 15
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      - name: Benchmark CLI performance

        run: |
          cd qa-automation
          echo "ðŸš€ Benchmarking CLI performance..."

          # Test each framework 3 times and measure average
          for framework in "vite" "nextjs"; do
            echo "Testing $framework performance..."
            total_time=0
            
            for i in {1..3}; do
              start_time=$(date +%s%N)
              node ../bin/react-kickstart.js "benchmark-$framework-$i" --yes --framework $framework --no-autostart >/dev/null 2>&1
              end_time=$(date +%s%N)
              
              duration=$(( (end_time - start_time) / 1000000 )) # Convert to milliseconds
              total_time=$(( total_time + duration ))
              echo "  Run $i: ${duration}ms"
              
              # Cleanup
              rm -rf "benchmark-$framework-$i"
            done
            
            avg_time=$(( total_time / 3 ))
            echo "  Average for $framework: ${avg_time}ms"
            echo "BENCHMARK_${framework^^}_AVG=${avg_time}" >> $GITHUB_ENV
          done

      - name: Comment performance results
        uses: actions/github-script@v7
        with:
          script: |
            const viteAvg = process.env.BENCHMARK_VITE_AVG;
            const nextjsAvg = process.env.BENCHMARK_NEXTJS_AVG;

            const comment = `## âš¡ Performance Benchmark Results

            **CLI Generation Speed:**
            - ðŸš€ Vite: ${viteAvg}ms average
            - â–² Next.js: ${nextjsAvg}ms average

            *Benchmark includes project generation, dependency installation, and initial setup.*

            ${viteAvg < 30000 && nextjsAvg < 45000 ? 'âœ… Performance is within acceptable range' : 'âš ï¸ Performance may need optimization'}`;

            // Find recent commits to comment on
            const commits = await github.rest.repos.listCommits({
              owner: context.repo.owner,
              repo: context.repo.repo,
              per_page: 1
            });

            if (commits.data.length > 0) {
              github.rest.repos.createCommitComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                commit_sha: commits.data[0].sha,
                body: comment
              });
            }
